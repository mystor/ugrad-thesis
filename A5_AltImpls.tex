% Appendix 2
\glsresetall % reset the glossary to expand acronyms again
\chapter{Alternative Implementations}\label{ch:AltImpls}

The first mechanism for implementing this system would be to follow in the
footsteps of mypy. Mypy uses Python 3's Function Annotation
syntax \cite{pythonfuncannot} to allow code written for mypy to also be valid
python 3 code. On one hand, this is an advantage, as it means that no
translation work needs to be done. Unfortunately, to make typing more explicit
and reduce programmer error, we also want to add annotations to variable
declarations, and other places which are not covered by the function annotation
syntax. However, we will likely re-use the function annotation syntax for this
thesis as a standard way to write function types and return values, as it
already fits with the rest of python's syntax, and is an advanced enough feature
that it is reasonable to remove from the dialect created for this thesis.

Languages based very closely on the semantics of other languages, such as
TypeScript mentioned above, are often implemented through source-to-source
compilers, or transpilers. The transpiler acts like a compiler, parsing and
analyzing the code written in the dialect. However, instead of performing
translation to assembly or machine code directly, transpilers emit code in the
base language. For example, the TypeScript compiler reads in TypeScript code,
analyzes it, and then emits valid ECMAScript code. This has an awesome advantage
of eliminating the requirement of implementing a full language implementation of
the base language, as the semantics can be derived from an existing
implementation of that language. We plan to use that technique for this thesis
in order to avoid re-implementing much of python, and to ensure that we maintain
accurate semantics.

One approach to the development of this dialect would be to take the same
approach as Microsoft. The dialect’s transpiler would be written, much like any
other compiler, from scratch using standard compiler technologies. The code
generation stage would be the only difference between the transpiler and any
other compiler, in that instead of generating machine code, it would be aiming
to generate code in the base language which as closely maps to the original code
as possible, such that errors generated by the interpreter map back accurately
to the source code for debugging purposes. However, writing a complete compiler
from scratch is often unnecessary as people have developed awesome tooling for
implementing language dialects already.

An interesting and useful tool for defining new languages without writing a
complete compiler is the Spoofax
workbench \cite{spoofaxpaper}, \cite{spoofaxweb}. Spoofax is a complete
tool-chain for defining the syntax and semantics of a language. Syntax is
defined using the Syntax Definition Formalism (SDF3) specification format, which
allows for complete language grammars to be described using a declarative
formation. Semantic analysis is handled using a variety of tools, including Name
Binding Language (NaBL), which handles semantic analysis of name binding, such
as namespaces, declarations, and references; and Type Specification Language
(TS), which allows for a declarative definition of a language's type system.
Finally, language developers translate the complete, type-checked and analyzed
language into a base language, such as Java, using the Stratego Transformation
Language \cite{strategoweb}, which declaratively maps AST structures in the new
language to structures in an existing language, such as Java. Spoofax also
provides a comprehensive testing story, with the Spoofax Testing Language (SPT)
which can be used to test parsing rules, error and warnings in language
productions, and transformation outputs. Finally, Stratego also uses these
analyses to generate IDE tooling for the languages in question - languages built
with Stratego get syntax highlighting, go to definition, and other useful tools
almost for free.

The biggest, and most impressive, advantage of the Spoofax toolbox for language
definition is it’s polish and integration with the Eclipse IDE. Languages
defined in spoofax are developed in a complete development environment, which
includes features such as inspection and debugging of intermediate data
structures, as well as IDE integration for the generated language. The IDE
integration is especially useful for a learning project, as it gives new
developers the tools they need to explore and learn about their new programming
language without having to memorize function and type names from libraries.

The Turing Extender Language (TXL) \cite{txlpaper}, \cite{txlweb} is another
system for defining new languages in terms of base languages. TXL’s processing
is split into two distinct steps. First, in the syntactic phase, the dialect
language is parsed, and an AST of the dialect language is generated. This
dialect AST is then passed to the semantic phase, which describes the semantic
meaning of the dialect language through a series of transformations into the
base language AST. This AST is then written out, and can be compiled or
executed.

Much like Spoofax, TXL is intended to be used with known syntax definitions for
a base language, such as the Turing programming language. TXL supports tooling
for modifying portions of a base language’s syntax, allowing for dialect
languages to be easily extended off of the syntactic structures of the base
language. However, TXL is also capable of describing complete new languages,
with completely different syntactic properties. In addition, by using multiple
passes which annotate expressions with their type, it is possible for TXL to
perform complex semantic type analysis. In the case of a dynamic language, a TXL
definition of the base language, such as python, would be developed, followed by
the addition of the typing syntactic structures in the dialect.

Much like Spoofax, and despite the use of Turing in its name, TXL is a generic
framework for extending languages, and is not specific to the Turing Programming
Language. With TXL it is possible to manipulate and generate code in many
different programming languages.

Unlike Spoofax, TXL doesn’t provide an immediate tool for integration into an
IDE, but with a language like Python, an extension to the open-source IDE IDLE
to transform the language before running it shouldn’t be too difficult to add,
and could create a very integrated experience which will be familiar when the
student moves away from the statically typed subset to taking advantage of the
entire language.

The Rascal meta-programming Language \cite{rascalpaper}, \cite{rascalweb} is an
attempt to develop a standard language for all forms of program analysis,
parsing, and code generation. It provides a single set of abstractions for
creating static analyses for existing programming languages, performing
automated code refactorings with understanding of the semantic meaning of the
underlying code, and generating DSLs (Domain Specific Languages) which compile
to an underlying language. Like Spoofax, the programs written in Rascal can also
be connected to an IDE, to provide auto-completion and features like
go-to-definition to developers in the IDE, which is a very useful feature for
developing a teaching language. The unified structure provides an advantage for
the development of

In addition Rascal comes with parsers for existing languages, such as Java,
which can allow for the easy creation of extra tooling or language features
which build on top of existing language parsers, limiting the amount of
development which is necessary in order to add a new feature to a programming
language. This means that new features can be added to a language like Java with
relative ease, by defining the new language feature in terms of features in the
base language. While there are existing libraries for parsing many languages,
Python is supported, which means that a new library will have to be implemented
in Rascal for this project if Python is chosen as the target language, and
Rascal as the transpilation framework.

If we’re OK with building the analysis upon other languages, we might want to
take a look at the Scheme programming language \cite{schemebook}. The Scheme
programming language takes a different approach to language extension then the
approaches taken by Spoofax and TXL: namely, instead of starting from a complex
language syntax and extending it, it starts with an extremely simple syntax -
that of S-expressions, and defines the entire language in terms of them. In
scheme, the statement `if` looks like `(if THEN ELSE)` which is identical to the
appearance of a function call with two arguments `(func ARG1 ARG2)`. This
uniform syntax allows for very easy to use macros. Scheme defines macros like a
special function type which is evaluated outside-in at “compile-time”. When the
macro is invoked, the AST of the arguments is provided to the macro’s
implementation, which is then given the opportunity to emit arbitrary code as
output. In addition, Scheme comes with a powerful macro-by-example system called
define-syntax, which allows for syntactic patterns to be defined and matched
against, along with simple syntax for defining the resulting meaning in a
declarative manner. In addition, scheme macros implement “hygiene”, which means
that they can internally use identifiers without the risk of the identifiers
conflicting with identifiers found at the use-sites of the
macro \cite{schemesyntaxpaper}.

This is a very powerful approach, as it allows defining brand new language
constructs which look nearly identical to the built-in language constructs
directly within the language itself. Unfortunately, it is a relatively poor tool
for defining brand new languages. While it is possible to define many new
language concepts, the syntax is constrained to use s-expressions, and certain
keywords are defined by the core language implementation, and cannot be
re-defined by macros. Thus, scheme’s macro system is a very powerful system for
language extension, but is incapable of defining arbitrary new languages.

However, we are not aiming to create an arbitrary new language, we are looking
to extend the language with type checking. Unfortunately Scheme doesn’t come
with the best tools for the job here, as the program as a unit is not implicitly
wrapped in a macro, which means that it is not possible in the generic case to
perform full-program transformations. However, if an implicit outer macro
invocation was created, it would be possible to define a macro transformation
which would generate valid Scheme, and which would perform type checking.
Unfortunately, however, much of the benefits of the modular scheme approach
would not be able to be taken, and the macro would effectively amount to a
partial compiler implementation.

In addition, Scheme has the disadvantage of, while it has a somewhat significant
library collection, lacking the industry presence which we are looking for to
make the language more appealing to students, and a better stepping stone to
real projects. These factors make Scheme a poor choice for this project.

The Julia Programming Language \cite{juliapaper}, \cite{juliaweb}, much like in
the Scheme programming language, allows programmers to manipulate the AST of
Julia programs. However, unlike Scheme, Julia doesn’t limit its program syntax
to only take the form of s-expressions, rather the syntax takes the form of
arbitrary Julia syntactic structures. The syntax for Julia’s macro expressions,
however, are limited. Macro names must be prefixed by the @ sign, distinguishing
them from native language constructs. In addition they must take one of two
forms, either `@name a1 a2 …` or `@name(a1, a2, …)`. Much like Scheme’s macros,
Julia’s macros are hygienic, and do not pollute the call site’s identifier
space. The arguments which are passed to the the Julia function take the form of
julia expression AST nodes.

In addition to Julia’s macros, Julia also supports generated functions.
Generated functions are formed using the @generated macro, which annotates a
function declaration. Wherever a generated function is called, the generated
function’s body is called, and passed the computed types of the arguments. The
function is then able to generate custom logic which will be executed when the
function is called at run-time. This is very similar in concept to generic
functions in other languages such as C++, except instead of writing the code
generation logic in a declarative system such as C++’s template syntax,
generated function code generation logic is instead written directly in Julia.
This is extremely useful for writing complex generic functions which need to
have custom functionality based on types which would be difficult or impossible
to express under a more limited system. Combined with Julia’s macros, this
provides Julia with a very powerful meta-programming story. However, as is the
case with Scheme, Julia is limited in that it is not possible to define a
completely distinct language with only these constructs, as the syntactic
transformations which may be performed are limited.

Julia is also a type-safe language, and has remarkably good error messages. Many
of the type-checking semantics and error messages which Julia provides could be
used as guides for the target behavior of whatever mechanism is chosen.
Unfortunately, we do not believe that Julia does a good job of filling this
role, as it lacks the libraries and production usage of other languages like
Python due to its young age. However, we could see Julia potentially being used
in the future for teaching programming concepts.

Another area of CS research related to program transformation is the area of
Aspect Oriented Programming. In the Aspect Oriented Programming paradigm,
programs are described in two parts: the main program, which describes the
primary business logic, and a set of “cross-cutting concerns” or
aspects \cite{aspectpaper}. The aspect interpreter or compiler then executes the
main program, interleaving the aspect logic whenever certain “pointcuts”
(language structures) are reached in execution. For example, an aspect may bind
to method calls on a database object, causing a logging operation to take place
whenever one of these calls is made. This technology can also be used by
language developers which wish to make changes to the basic semantics of the
target language. By binding and executing different instructions before and
after pointcuts in the program using an aspect oriented interleaver, the
semantics of individual instructions can be modified without needing to change
the language’s structure, allowing new languages to be defined with different
semantics, but the same syntax, as other languages.

This form of transformation, however, is not the type of language transformation
that we need for this project. Our goal is to avoid changing the semantics of
the language while providing additional analysis at compile time, which takes
advantage of new syntax. For this reason, the technologies used by aspect
oriented programming will be unlikely to be very useful in this project.

